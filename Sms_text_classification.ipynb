{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ardaolmez/Sms_Classification/blob/main/Sms_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(train_file_path, sep=\"\\t\", header=None, names=['y', 'x'])\n",
        "df_train.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(test_file_path, sep=\"\\t\", header=None, names=['y', 'x'])\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df_train['y'].astype('category').cat.codes\n",
        "y_test  = df_test['y'].astype('category').cat.codes"
      ],
      "metadata": {
        "id": "zYqdPaQhSyE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_eng = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_txt(txt):\n",
        "    txt = re.sub(r'([^\\s\\w])+', ' ', txt)\n",
        "    txt = \" \".join([lemmatizer.lemmatize(word) for word in txt.split()\n",
        "                    if not word in stopwords_eng])\n",
        "    txt = txt.lower()\n",
        "    return txt\n"
      ],
      "metadata": {
        "id": "sOCh9Fu1Rza6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train['x'].apply(lambda x: clean_txt(x))\n",
        "X_train[:5]\n",
        "X_test = df_test['x'].apply(lambda x: clean_txt(x))"
      ],
      "metadata": {
        "id": "zfohZhdVSBGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "# Define the tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Keep top 1000 frequently occurring words\n",
        "max_words = 1000\n",
        "max_len = 500\n",
        "\n",
        "\n",
        "# Tokenize the training data\n",
        "tokenized_text = [tokenizer(text) for text in X_train]\n",
        "\n",
        "# Count the word frequencies\n",
        "word_freq = Counter()\n",
        "for tokens in tokenized_text:\n",
        "    word_freq.update(tokens)\n",
        "\n",
        "# Sort the words by frequency in descending order\n",
        "sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Keep only the top 1000 words\n",
        "top_words = [word for word, _ in sorted_words[:max_words]]\n",
        "\n",
        "# Convert tokens to indices using the top words\n",
        "word2idx = {word: idx for idx, word in enumerate(top_words)}\n",
        "print(len(word2idx))\n",
        "# Convert each tokenized text to indices\n",
        "sequences = []\n",
        "for tokens in tokenized_text:\n",
        "    seq = [word2idx[token] for token in tokens if token in word2idx]\n",
        "    sequences.append(seq)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "padded_sequences = pad_sequence([torch.tensor(seq) for seq in sequences], batch_first=True, padding_value=0)\n",
        "\n",
        "# Convert the sequences to PyTorch tensors\n",
        "sequences_matrix = padded_sequences\n",
        "print(sequences_matrix.shape)\n",
        "print(X_train[:5])\n",
        "\n",
        "\n",
        "y_train_tensor = torch.tensor(y_train.values)\n",
        "y_test_tensor = torch.tensor(y_test.values)\n",
        "\n",
        "# Print the shape of the tensors\n",
        "print(y_train_tensor.shape)\n",
        "print(y_test_tensor.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "9XLj_kJhSMGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg, pos = np.bincount(y_train_tensor)\n",
        "\n",
        "# Calculate total samples.\n",
        "total = neg + pos\n",
        "\n",
        "# Calculate the weight for each label.\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = torch.tensor([weight_for_0,weight_for_1 ]).float()\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ],
      "metadata": {
        "id": "VN58bU1N-bEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class sms(nn.Module):\n",
        "    def __init__(self, max_words, max_len):\n",
        "        super(sms, self).__init__()\n",
        "        self.embedding = nn.Embedding(max_words, 32)\n",
        "        self.lstm = nn.LSTM(32, 64)\n",
        "        self.dense1 = nn.Linear(64, 128)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.dense2 = nn.Linear(128, 2)\n",
        "        #self.elu = nn.Sigmoid()\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.layer1=torch.nn.LayerNorm(128)\n",
        "        self.layer2=torch.nn.LayerNorm(1)\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        output, (h, c) = self.lstm(x)\n",
        "        output = output.squeeze(0)\n",
        "        x = self.dense1(output)\n",
        "        x=self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.dense2(x)\n",
        "        #logits = self.layer2(x)\n",
        "        #logits = self.elu(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = logits.shape\n",
        "            logits=logits.mean(1)\n",
        "            targets = targets.long()\n",
        "            loss = F.cross_entropy(logits,targets,weight=class_weight)\n",
        "        return logits, loss\n",
        "\n",
        "# Example usage\n",
        "\n",
        "\n",
        "# Create an instance of the model\n",
        "model = sms(max_words, max_len)\n",
        "\n",
        "# Perform a forward pass to compute the loss\n",
        "outputs = model(sequences_matrix)\n"
      ],
      "metadata": {
        "id": "hW-leLBRVS40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "block_size = 32\n",
        "eval_iters = 200\n",
        "batch_size = 64\n",
        "eval_interval=1000\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = sequences_matrix\n",
        "    ix = torch.randint(len(data), (batch_size,))\n",
        "    x = torch.stack([data[i] for i in ix])\n",
        "    y = torch.stack([y_train_tensor[i] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "for iter in range(10000):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    #if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "     #   losses = estimate_loss()\n",
        "      #  print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits,loss= model(xb,yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if iter % eval_interval == 0 or iter == 100000 - 1:\n",
        "      logits,loss= model(sequences_matrix,y_train_tensor)\n",
        "      print(f\"step {iter}: train loss {loss:.4f}\",logits[:10],y_train_tensor[:10])\n",
        "\n"
      ],
      "metadata": {
        "id": "7ws_ZrRV61cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "outputs": [],
      "source": [
        "# function to predict messages based on model\n",
        "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
        "def predict_message(pred_text):\n",
        "    pred_train = [pred_text]\n",
        "    tokenized_text = [tokenizer(text) for text in pred_train]\n",
        "    sequences = []\n",
        "    for tokens in tokenized_text:\n",
        "        seq = [word2idx[token] for token in tokens if token in word2idx]\n",
        "        sequences.append(seq)\n",
        "    padded_sequences = pad_sequence([torch.tensor(seq) for seq in sequences], batch_first=True, padding_value=0)\n",
        "    logits = model(padded_sequences)\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]  # Assuming the model returns a tuple of (logits, ...)\n",
        "    logits = torch.tensor(logits)\n",
        "    counts = torch.exp(logits)\n",
        "    probs = counts / counts.sum(1, keepdims=True)\n",
        "\n",
        "    return (probs[0][0], (\"ham\" if probs[0][0] < 0.5 else \"spam\"))\n",
        "pred_text = \"how are you doing today?\"\n",
        "\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}